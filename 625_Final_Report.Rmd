---
title: "Biostats 625 Final Project - US 2019 Census Data"
author: "Group 4 members: Ting Gong, Lap Sum Chan, Margaret Prentice"
date: "December 20, 2020"
output:
  pdf_document: default
  html_document: default
  rmarkdown::pdf_document:
    fig_caption: yes
    includes: null
subtitle: \textit{https://github.com/Gongting811/625FinalProject}
header-includes:
- \usepackage[utf8]{inputenc}
- \usepackage{listings}
- \usepackage{amsmath}
- \usepackage{bm}
- \usepackage{bbm}
- \usepackage{amssymb}
- \usepackage{graphicx}
- \usepackage{multirow}
- \usepackage{caption}
- \usepackage{color}
- \usepackage{array}
- \usepackage{tabu}
- \usepackage{mathtools}
- \usepackage{bbold}
- \usepackage{array}
- \usepackage{mathrsfs}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(ggplot2)
```

## 1. Introduction

The problem of income inequality has been of great concern in the recent years. Large differences in annual incomes may be the result of a combination of factors such as education level, age, gender, occupation, race, etc. This project aims to conduct a comprehensive analysis to highlight the key factors that are necessary in improving an individual's income. Such analysis would help to set focus on the important areas which can significantly improve the income levels of individuals and thus help to provide a guidance for the individual who wants to make some changes to improve their income level. After identifying the important predictors for income, several binary classifiers are trained to predict whether an individual's annual income in 2019 falls in the income category of either greater than or equal to 60,000 USD or less than 60,000 USD using the dataset extracted from the 2019 Census Bureau database.

## 2. Data Pre-processing

The demographic and income data in this report are from the Current Population Survey (CPS) Annual Social and Economic Supplement (ASEC) conducted by the US Census Bureau in 2019. The US Census Bureau collects data and publishes estimates on income and poverty each year to evaluate national economic trends as well as to understand their impact on the wellbeing of households, families, and individuals. 

***Dataset Extraction***

Our dataset were extracted from the original individual-level ASEC dataset, which contains 180101 individuals and 799 features in total. The biggest challenge was to encode all the categorical variables. Most of the variables are categorical ones, but they were all encoded numerically in the original dataset. Therefore, the first thing we did was to find them all by reading the documentations and encode them in the proper categorical form by hand. Next, we applied several conditions including `income` > 0, `age` > 15 and `age` < 80, `labor force status` == "working", `working hour` > 0 to the dataset. Then, to reduce the dimension and select related features from the raw data, we did the Backward Elimination with `income` variable as the response variable and all the remaining ones as covariates. As a result, a set of features were selected that contains demographic variables such as `age`, `race`, `sex`, `academic degree`, `marital status`, `ethnic`, and `region`; employment variables such as `labor force status`, `working hours`, `worker classes`, `major industry` and `major occupation`; health-related variables such as `total medical expenditures` and `health status`. Finally, we merged similar columns for several categorical variables, dropped all the outliers and eventually got our census19 dataset.

```{r include=FALSE}
census19 <- read.table("census19.csv", header = T, sep = ',')
```


***Exploratory Data Analysis***

First, we plot a histogram and a boxplot of `age` versus `income level`. We can see from the graphs below that the `age` variable has a wide range and variability, and the percentage of people who make above $60000 peaks out at roughly 35% between ages 35 and 60. The boxplot shows that individuals who have higher incomes tend to be older than those who don't. This implies that `age` might be a good predictor of `income level`.

```{r, echo=FALSE, fig.width = 3.5, fig.height=2}
opar <- par(no.readonly = T); par(mfrow = c(1,2))
ggplot(census19, aes(A_AGE)) + 
  geom_histogram(aes(fill = income_level), color = "black", binwidth = 1) +
  labs(x = 'Age', title = 'Age vs. Income Level') +
  theme(legend.position = "none")
ggplot(aes(x = income_level, y = A_AGE, fill = income_level), data = census19) + 
  geom_boxplot() + 
  labs(x = 'Income', y = 'Age') +
  ggtitle('Age vs. Income Level') +
  theme(legend.position = "none")
par(opar);
```

Next, we plot a histogram and a boxplot of `working hours` (per week) versus `income level`. It is shown from the graph that the highest frequency of `working hours` (per week) occurs at around 35-45 hours. The boxplot shows that individuals who have higher incomes tend to work longer than those who don't. 


```{r, echo=FALSE, fig.width = 3.5, fig.height=2}
opar <- par(no.readonly = T); par(mfrow = c(1,2))
ggplot(census19, aes(A_USLHRS)) + 
  geom_histogram(aes(fill = income_level), color = "black", binwidth = 10) +
  labs(x = 'Working Hours', title = 'Working Hours vs. Income Level') +
  theme(legend.position = "none")
ggplot(aes(x = income_level, y = A_USLHRS, fill = income_level), data = census19) + 
  geom_boxplot() + 
  labs(x = 'Income', y = 'Working Hours') +
  ggtitle('Working Hours vs. Income Level') +
  theme(legend.position = "none")
par(opar);
```

After visualizing the distribution of `income level` versus the above two continuous variables `age` and `working hours`, we then explored some categorical variables. It turned out that the following four variables `sex`, `work class`, `academic degree` and `region` are all likely to be good predictors.


```{r, include=FALSE}
levels(census19$A_HGA)[levels(census19$A_HGA) %in% c('Less than 1st grade', '1st,2nd,3rd,or 4th grade', '5th or 6th grade', '7th and 8th grade', '9th grade', '10th grade', '11th grade', '12th grade no diploma', 'High school graduate - high school diploma or equivalent')] = "high school or below"

levels(census19$A_HGA)[levels(census19$A_HGA) %in% c('Some college but no degree', 'Associate degree in college - occupation/vocation program', 'Associate degree in college - academic program')] = "college no degree"

levels(census19$A_HGA)[levels(census19$A_HGA) %in% c('Bachelor\'s degree (for example: BA,AB,BS)')] = "BS degree"

levels(census19$A_HGA)[levels(census19$A_HGA) %in% c('Master\'s degree (for example: MA,MS,MENG,MED,MSW , MBA)', 'Professional school degree (for example: MD,DDS,DVM,LLB,JD)')] = "MS degree"
                      
levels(census19$A_HGA)[levels(census19$A_HGA) %in% c('Doctorate degree (for example: PHD,EDD)')] = "PhD degree" 

```

```{r, echo=FALSE, fig.width = 3.5, fig.height=3}
opar <- par(no.readonly = T); par(mfrow = c(1,2))
ggplot(aes(x = A_CLSWKR), data = census19) + 
  geom_bar(aes(fill = income_level), color = "black", width = 0.3) + 
  ggtitle('Workclass with Income Level') + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1), legend.position="none") +
  labs(x='Workclass')

ggplot(aes(x = A_SEX), data = census19) + 
  geom_bar(aes(fill = income_level), color = "black", width = 0.2) + 
  ggtitle('Sex vs Income Level') + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1), legend.position = "none") +
  labs(x = 'Sex')
par(opar);
```
```{r, echo=FALSE, fig.width = 3.5, fig.height=3}
opar <- par(no.readonly = T); par(mfrow = c(1,2))
ggplot(census19, aes(MIG_DIV)) + 
  geom_bar(aes(fill = income_level), color = "black") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1)) +
  labs(x = 'Region', title = 'Regions vs. Income Level') +
  theme(legend.position = "none")
ggplot(aes(x = A_HGA), data = census19) + 
  geom_bar(aes(fill = income_level), color = "black", width = 0.5) + 
  ggtitle('Academic Degree vs Income Level') + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1), legend.position = "none") +
  labs(x = 'Degree')
par(opar);
```

Lastly, we split the dataset into 60%, 20% and 20% for training, validation and testing respectively after the EDA.
 
## 3. Methodology

For each of the models, we selected the best performing model based on their accuracy performance on the validation set.

**Logistic Regression**

Several models and interaction terms were considered for logistic regression. The final model included all features settled upon during the features extraction, as well as interaction terms between `sex` and `marital status` and between `worker classes` and `major industry`. These interaction terms increased the accuracy of the model on the validation set as well as increased the area under the curve (AUC) of the receiver operating characteristic (ROC) curve as compared to the base logistic regression model with all features and no interaction terms. 

```{r include = FALSE}
library(caret)
library(precrec)
train <- read.csv("train.csv")
validation <- read.csv("validation.csv")
test <- read.csv("test.csv")

train$earnbin <- as.factor(train$earnbin)
validation$earnbin <- as.factor(validation$earnbin)
test$earnbin <- as.factor(test$earnbin)

#Fitting base logistic regression model on training data
logisticfit_base <- glm(earnbin ~ A_AGE + PRDTRACE + A_SEX + A_HGA + A_MARITL + PEHSPNON + MIG_DIV + A_USLHRS + A_CLSWKR + A_MJIND + A_MJOCC + MOOP , data = train, family = "binomial")

#Fitting final logistic regression model on training data
logisticfit_final <- glm(earnbin ~ A_AGE + A_SEX + PRDTRACE + A_HGA + A_SEX*A_MARITL + PEHSPNON + MIG_DIV + A_USLHRS + A_CLSWKR*A_MJIND + A_MJIND + A_MJOCC + MOOP + HEAbin, data = train, family = "binomial")

#Generating prediction on validation data for the two models
pred_base <- predict(logisticfit_base, newdata = validation, type = "response")
pred_final <- predict(logisticfit_final, newdata = validation, type = "response")

#Generate statistics associated to 2 x 2 table for the two models
stat_base <- confusionMatrix(as.factor(as.numeric(pred_base >= 0.5)), validation$earnbin, positive = "1")
stat_final <- confusionMatrix(as.factor(as.numeric(pred_final >= 0.5)), validation$earnbin, positive = "1")

print(stat_base$overall[1]) #80.26% accuracy for base model in validation set
print(stat_final$overall[1]) #80.52% accuracy for final model in validation set

#Generate AUC for the two models
auc_base <- evalmod(scores = pred_base, labels = validation$earnbin)
auc_final <- evalmod(scores = pred_final, labels = validation$earnbin)

print(attributes(auc_base)$aucs[1,"aucs"]) #0.8560 AUC under ROC
print(attributes(auc_final)$aucs[1,"aucs"]) #0.8575 AUC under ROC
```

**Random Forest**

For the random forest model, it was based on the `randomForest` package and we varied the number of variables randomly sampled as candidates at each split (`mtry`). We tried `mtry` from 2 to 10, and `mtry` = 4 had the best accuracy in the validation set. Notice that the difference in accuracy between the model with highest accuracy (`mtry` = 4) and that of lowest accuracy (`mtry` = 9) was only about 0.007 or 0.7%.

```{r include = FALSE}
library(randomForest)
rflist <- list() #List for holding random forest models (mtry = 2 to 10)
rfpredlist <- list() #List for holding prediction results on validation set (mtry = 2 to 10)
accuracy <- rep(NA, 10) #Vector for holding accuracy values on validation set (mtry = 2 to 10)

############ Running this markdown block can be skipped to save time, load RDS data instead ############
for (i in 2:10) {
  set.seed(20201215)
  #Fitting random forest models on training data
  rflist[[i]] <- randomForest(earnbin ~ A_AGE + PRDTRACE + A_SEX + A_HGA + A_MARITL + PEHSPNON + MIG_DIV + 
                                A_USLHRS + A_CLSWKR + A_MJIND + A_MJOCC + MOOP + HEAbin, data = train,
                              mtry = i, importance = T)
}

for (i in 2:10) {
  #Generating prediction on validation data for the random forest models
  rfpredlist[[i]] <- predict(rflist[[i]], newdata = validation, type = "class")
}

for (i in 2:10) {
  #Calculate the accuracy for the random forest models
  accuracy[i] <- confusionMatrix(rfpredlist[[i]], validation$earnbin, positive = "1")$overall[1]
}

saveRDS(rflist, "rflist.RDS")
saveRDS(rfpredlist, "rfpredlist.RDS")
saveRDS(accuracy, "accuracy.RDS")
```

```{r include = FALSE}
############ Results from above were saved in RDS and can be loaded directly for convenience ############
rflist <- readRDS("rflist.RDS")
rfpredlist <- readRDS("rfpredlist.RDS")
accuracy <- readRDS("accuracy.RDS")

print(which.max(accuracy)) #mtry = 4 gives the maximum accuracy in validation set
print(max(accuracy, na.rm = TRUE)) #80.06% accuracy

print(which.min(accuracy)) #mtry = 9 gives the minimum accuracy in validation set
print(min(accuracy, na.rm = TRUE)) #79.33% accuracy

mtry <- which.max(accuracy) #selected mtry parameter, saved for later use for testing final model
```
**XGBoost**

We used the `xgboost` package for the XGBoost tree booster model. We tried a wide range of tuning parameters, which included the learning rate `eta` = 0.025, 0.05, 0.1 and 0.3, the maximum depth of a tree `max_depth` from 2 to 6, and varied the number of boosting iterations from 300 to 1000. The model with learning rate 0.025, maximum depth of a tree = 6 and number of boosting itearions = 300 was chosen as it had the lowest validation error (18.63%) amongst all the parameter grids we searched. In other words, the XGBoost model achieved an accuracy of 81.37% in the validation set.
```{r include = FALSE}
library(dummies)
library(varhandle)
library(xgboost)
train <- train[,-c(1,2)]
validation <- validation[,-c(1,2)]
test <- test[,-c(1,2)]

#Creates covariate matrix objects that fulfill XGBoost requirement (data matrix)
new_tr <- dummy.data.frame(train[,-14]) #Training
new_va <- dummy.data.frame(validation[,-14]) #Validation
new_va2 <- subset(new_va, select = colnames(new_tr)) #Validation with columns arranged in same order
new_te <- dummy.data.frame(test[,-14]) #Testing
new_te2 <- subset(new_te, select = colnames(new_te)) #Testing with columns arranged in same order

dtrain <- xgb.DMatrix(data.matrix(new_tr), label = unfactor(train[,14]))
dvalid <- xgb.DMatrix(data.matrix(new_va2), label = unfactor(validation[,14]))
dtest <- xgb.DMatrix(data.matrix(new_te2), label = unfactor(test[,14]))

#Final sets of parameter: max_depth = 6, eta = 0.025 we landed on after tuning
param <- list(max_depth = 6, eta = 0.025, verbose = 1, nthread = 2,
              objective = "binary:logistic", eval_metric = "error")

#watchlist allows printing of training and validation error while running the model 
watchlist <- list(train = dtrain, eval = dvalid)

set.seed(20201215)
#Fitting XGBoost on training data based on selected tuning parameters
bst <- xgb.train(params = param, data = dtrain, nrounds = 300, watchlist = watchlist)
print(bst$evaluation_log$eval_error[300]) #18.63% error, i.e. 81.37% accuracy
saveRDS(bst, "bst.RDS")
```

## 4. Result

***Feature Importance***
Based on our training data, we found that the `major occupation` feature was most important with about 0.036 mean decrease in accuracy when `major occupation` is permuted after training and before prediction, while the `health status` feature was least important with about 0.0005 mean decrease in accuracy when it is permuted according to the results of our random forest model. The `academic degree` and `working hours` were second and third most important, both with about 0.023 mean decrease in accuracy. `age`, `total medical expenditures`, `major industry`, and `sex` each had mean decrease in accuracy ranging from about 0.016 to 0.012, while all other features had mean decrease in accuracy less than 0.01.

```{r include = FALSE}
#Dataframe for plotting variance importance
rfimp <- data.frame(rownames(rflist[[mtry]]$importance), rflist[[mtry]]$importance[,"MeanDecreaseAccuracy"])
colnames(rfimp) <- c("VariableNames", "MeanDecreaseAccuracy")
ggplot(rfimp, aes(x = MeanDecreaseAccuracy, y = reorder(VariableNames, MeanDecreaseAccuracy))) + geom_bar(stat = "identity") + labs(title = "Variable Importance Plot", x = "Mean decrease accuracy", y = "Predictors")
```

***Model Comparison***
Finally, we applied the previously learned models to the test data to get a relatively unbiased evaluation of each model. The prediction accuracy of logistic regression, random forest and XGBoost models on the test data were 80.04%, 80.06% and 80.98%, respectively. Similar result could also be seen in the ROC curve comparing the three methods. We can see that there is a huge overlap between the logistic regression and random forest and XGBoost has only a tiny margin above the two methods. Indeed, the corresponding AUC for logistic regression, random forest and XGBoost are 0.8476, 0.8450 and 0.8593. Notice that random forest had an even worse performance when measured using AUC.
```{r include = FALSE}
library(pROC)
library(RColorBrewer)

#Generating prediction on test data based on selected models

#Logistic regression
pred_logistic <- predict(logisticfit_final, newdata = test, type = "response")
stat_logistic <- confusionMatrix(as.factor(as.numeric(pred_logistic >= 0.5)), test$earnbin, positive = "1")
print(stat_logistic$overall[1]) #80.04% accuracy for logistic regression

#Random forest
pred_rf <- predict(rflist[[mtry]], newdata = test, type = "class")
stat_rf <- confusionMatrix(pred_rf, test$earnbin, positive = "1")
print(stat_rf$overall[1]) #80.06% accuracy for random forest

#XGBoost
stat_xgboost <- confusionMatrix(as.factor(as.numeric(predict(bst, dtest) >= 0.5)), test$earnbin, positive = "1")
print(stat_xgboost$overall[1]) #80.98% accuracy for XGBoost

#Obtain ROC curves objects (contains AUC) for models
logistic_roc <- roc(test$earnbin ~ pred_logistic)
pred_rf2 <- predict(rflist[[mtry]], newdata = test, type = "prob")
rf_roc <- roc(test$earnbin ~ as.data.frame(pred_rf2)[,2])
xgboost_roc <- roc(test$earnbin ~ predict(bst, dtest))

print(logistic_roc) #AUC = 0.8476
print(rf_roc) #AUC = 0.845
print(xgboost_roc) #AUC = 0.8593

#Extract sensitivity and specificity for models
logistic_df <- data.frame(logistic_roc$specificities, logistic_roc$sensitivities, "Logistic Regression")
rf_df <- data.frame(rf_roc$specificities, rf_roc$sensitivities, "Random Forest")
xgboost_df <- data.frame(xgboost_roc$specificities, xgboost_roc$sensitivities, "XGBoost")

#Set common column names before merging
colnames(logistic_df) <- c("Specificity", "Sensitivity", "Methods")
colnames(rf_df) <- c("Specificity", "Sensitivity", "Methods")
colnames(xgboost_df) <- c("Specificity", "Sensitivity", "Methods")

#Merge the roc curve values into one dataframe for plotting
roc_df <- rbind(logistic_df, rf_df, xgboost_df)
ggplot(roc_df) + geom_line(aes(x = 1 - Specificity, y = Sensitivity, color = Methods)) + labs(title = "ROC curve") + scale_color_manual(values = ("#CC79A7", "#E69F00", "#56B4E9"))
```



